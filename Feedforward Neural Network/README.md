## Feedforward Neural Network   
* Backpropagation ( Computes the gradient of Loss function ) Learning using user-specified optimizer  
    * In each epoch, it starts by randomly shuffling the training data, and then partitions it into mini-batches. 
    * Then for each mini-batch we apply a single step of gradient descent, which updates the network weights and biases. 
* [customdl](https://pypi.org/project/customdl/) package
