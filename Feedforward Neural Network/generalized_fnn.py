# -*- coding: utf-8 -*-
"""generalized_fnn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wm9zwn_hjt-EZsIDVYOm2oYb6BJDvFFR
"""

import numpy as np
import random
from activation_functions import activation_function
from loss_functions import loss_function

# Dense Layer
class Dense_Layer:

  # Initialize weights and biases
  def __init__(self, n_inputs, n_nodes, activation_type):
    # He initialization
    self.weights = np.random.randn(n_nodes, n_inputs) * np.sqrt(2 / n_nodes)
    # Zero initialization
    self.biases = np.zeros((1, n_nodes))

    self.n_nodes = n_nodes
    self.activation_type = activation_type

  # forward pass 
  # activation type: type of activation function
  def forward(self, x):
    for b, w in zip(self.biases, self.weights): 
      x = activation_function(self.activation_type, np.dot(w, x) + b)
    return x

  # Retrieve layer parameters
  def get_parameters(self):
    return self.weights, self.biases

class FNN():

  # x - list of inputs
  def __init__(self, x, loss_type):
    self.x = x
    self.nn = list()
    self.n_layers = 0
    self.n_inputs = len(x)
    self.weights = list()
    self.biases = list()
    self.activations = list()
    self.loss_fn = loss_type

  # display fnn
  def display(self):
    print("Inputs: ", self.x)
    print("Weights: ", self.weights)
    print("Biases: ", self.biases)
    print("Activations: ", self.activations)
    print("Layers: ", self.n_layers)

  # add layer to fnn
  def add(self, n_nodes, activation_type):
    if self.n_layers == 0:
        layer = Dense_Layer(self.n_inputs, n_nodes, activation_type)

        weights, biases = layer.get_parameters()
        self.weights.append(weights)
        self.biases.append(biases)    
        self.activations.append(layer.forward(self.x))
    else:
        layer = Dense_Layer(self.nn[-1].n_nodes, n_nodes, activation_type)

        weights, biases = layer.get_parameters()
        self.weights.append(weights)
        self.biases.append(biases)    
        self.activations.append(layer.forward(self.activations[-1]))
    
            
    self.nn.append(layer)
    self.n_layers += 1